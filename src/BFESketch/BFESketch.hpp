//
//  BFESketch.hpp
//  braindust
//
//  Created by Francesco Da Dalt on 22.04.24.
//

#ifndef McmcSketch_hpp
#define McmcSketch_hpp

#include "InitialPoint.hpp"
#include "GibbsSamplerEngine.hpp"
#include "GibbsSamplerContainer.hpp"
#include "HyperplaneSensor.hpp"
#include "LatentBase.hpp"

#include <iostream>
#include <thread>
#include <omp.h>

// hypergrid_type defines the strudture of the LDPC code used to construct H and thus defines H.
// key_type is the type of the keys
// support_type is the type of the values i.e. frequencies
// num_threads is the number fo threads used for query-operations
// fPlusOne is equal to f + 1 where f is the number of auxiliary RVs used to perform level-2 sampling
template <typename hypergrid_type, typename key_type, typename support_type, size_t num_threads, size_t fPlusOne>
class BFESketch {

//	Number of memory cells allocated for the primary sketch
	const size_t primary_memory_cells;
//	Approximate number of memory cells allocated for the secondary sketch
	const size_t secondary_memory_cells;
//	A multidimensional lattice that captures the LDPC code used for the matrix H
	const hypergrid_type hg;
//	The object that contains the on-line part of the sketch
	using senssys_type = HyperplaneSensor<key_type, support_type, hypergrid_type>;
	senssys_type sensor;
//	The latent base \Tilde{N} used for level-1 MCMC
	using hlbd_type = HyperplaneLatentBase<hypergrid_type, support_type>;
	const hlbd_type hlbd;
//	The latent base N used for level-2 MCMC
	using plbd_type = DegenerateLatentBase<fPlusOne, support_type>;
	const plbd_type plbd;
//	The sampling engine for level-1 MCMC
	using gse_type = GibbsSamplerEngine<hlbd_type, support_type>;
	const gse_type gse;
//	The sampling engine for level-2 MCMC
	using gsesi_type = GibbsSamplerEngineSingleItem<plbd_type, support_type>;
	const gsesi_type gsesi_aux;
//	The container for level-1 samples
	using gsc_type = GibbsSamplerContainer<num_threads, gse_type>;
	gsc_type gibbs_container;
//	The container for level-2 samples
	using gscsi_type = GibbsSamplerContainerSingleItem<gsesi_type>;
	std::array<gscsi_type, num_threads> gscsi_aux;
//	Flag that indicates whether level-2 is bypassed
	const bool bypassTopLevel = false;
//	Stats for emasuring memory
	std::vector<std::tuple<std::string, double>> memstats;
	
public:
	
//	Construct the sketch. max_level1_samples and max_level2_samples are used to pre-allocate space for potential samples to be generated.
//	Bypass top level means items are not hashed and are only mapped by modulo-operation to the counters. Also, when querying, we only return samples generated by the first level of MCMC
	BFESketch (const size_t nummemorycells,
			   const size_t max_level1_samples,
				const size_t max_level2_samples,
				const bool bypassTopLevel):
	primary_memory_cells(nummemorycells - (int) 4 * std::log2(nummemorycells)),
	secondary_memory_cells(nummemorycells - primary_memory_cells),
	hg(array_aux<hypergrid_type::num_axes>(primary_memory_cells - 2 * (hypergrid_type::num_axes - 1), 2)),
	sensor(hg, secondary_memory_cells),
	hlbd(hg),
	gse(&hlbd),
	gibbs_container(&gse, max_level1_samples),
	gsesi_aux(&plbd),
	bypassTopLevel(bypassTopLevel) {
		for (int i = 0; i < num_threads; i++) {new (gscsi_aux.data() + i) GibbsSamplerContainerSingleItem(&gsesi_aux, max_level2_samples);}
	}
	
//	Insert key-value pair into sketch
	inline void insert(const std::tuple<key_type, support_type> key_value) {
		sensor.insert(std::get<0>(key_value), std::get<1>(key_value), bypassTopLevel);
	}
	
//	Request queries for the frequencies of items with keys keys. Assuming a prior level2_prior for A and level1_prior for \Tilde{A}.
//	The algorithm attempts to return level2_samples * level1_samples samples in total.
//	The gibbs sampling can be tuned by defining burn-in period (basically number of initial samples to discard) and the period with which we store samples ( every level2_period'th sample generated during level-2 MCMC gets stored and returned, all other samples are discarded. Increases computational cost but improves quality of samples due to higher independence).
	template <typename level1_prior_type, typename level2_prior_type>
	auto batch_query (const std::vector<key_type>* const keys,
					  level1_prior_type* const level1_prior,
					  level2_prior_type* const level2_prior,
					  int level1_samples,
					  int level2_samples,
					  const int level1_period = 1,
					  const int level1_burnin = 0,
					  const int level2_period = 1,
					  const int level2_burnin = 0) {
		
//		Clamp to ensure numerical stability
		sensor.clamp_min(std::max(level1_prior->get_bounds().first, level2_prior->get_bounds().first) + 1e-6);
//		Retrive counters from sensor object
		const auto counters_eigen = Eigen::Map<const Eigen::Array<support_type, Eigen::Dynamic, 1>>(sensor.counter_ptr(), hg.num_counters);
//		Distinguish cases where we dont need to perform either level1 or level 2 samples and adjust such that we still return around level2_samples * level1_samples samples in total.
		if (hg.num_counters == hg.num_nodes and bypassTopLevel) {
			level1_samples = 1;
			level2_samples = 1;
		} else if (bypassTopLevel) {
			level1_samples *= level2_samples;
			level2_samples = 1;
		} else if (hg.num_counters == hg.num_nodes) {
			level2_samples *= level1_samples;
			level1_samples = 1;
		}
		
//		Estimate number of distinct keys
		const size_t cardianlity_estimate = sensor.get_distinct_estimate();
//		Total volume
		const support_type total_volume = sensor.get_total();
//		Average requency estimate
		const double average_frequency = ((double) total_volume) / cardianlity_estimate;
//		Fit prior A's scale such that its mean is average_frequency
		level2_prior->adjust_to_match_mean(average_frequency);
		
//		Average frequency of items in the d intermediate bins
		const double average_bin_frequency = ((double) total_volume) / hg.num_nodes;
//		Estimate probability of two items colliding in the d intermediate bins
		const double collision_probability = 1.0 / hg.num_nodes;
//		Estimate variance of the d intermediate counters. Based on law of total variance
		const double average_bin_variance = collision_probability * cardianlity_estimate * (level2_prior->variance(std::numeric_limits<support_type>::max()) + (1 - collision_probability) * std::pow(level2_prior->mean(std::numeric_limits<support_type>::max()), 2.0));
		
//		If we bypass level-2 MCMC, we assume there is one item for every one of the d bins and we adjust the level-1 prior to be the prior of A.
//		If we dont bypass level-2 then level-1 bins have expectation and variance according to formula.
		if (!bypassTopLevel) {level1_prior->adjust_to_match_mean_var(average_bin_frequency, average_bin_variance);} else {level1_prior->adjust_to_match_mean(average_frequency);}
		
//		Here we store the level-1 MCMC samples
		Eigen::Array<support_type, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor> firstLevel_samples(hg.num_nodes, level1_samples);
		
//		If we have as many counters as bins d then we assume the matrix H is an identity matrix and no level-1 MCMC is necessary.
//		If d is greater than the number of counters, we perform level-1 MCMC
		if (hg.num_counters != hg.num_nodes) {
			#ifndef NO_MOSEK
//			Find num_threads many random initial points for the level-1 MCMC. We then start num_threads many MCMC chains i parallel
			Eigen::Array<support_type, Eigen::Dynamic, Eigen::Dynamic> initial_points;
			multiple_random_initial_points<senssys_type, level1_prior_type, num_threads, num_threads>(&sensor,
																									  hg.num_nodes,
																									  hg.num_counters,
																									  &counters_eigen,
																									  &initial_points,
																									  1e-6,
																									  level1_prior,
																									  gibbs_container.get_rng(0));
//			Initialize num_threads-many MCMC chain to the found initial values
			gibbs_container.refresh(&initial_points);
//			Initial burn-in (parallel)
			gibbs_container.burnin(level1_prior, level1_burnin);
//			Sampling process (parallel)
		    gibbs_container.sample(level1_prior, level1_period, level1_samples);
//			Retrieve generates samples (parallel)
			gibbs_container.get_samples(&firstLevel_samples);
			#endif
		} else {
//			No MCMC required as the only possible value for the d bins is equal to the d counters
			firstLevel_samples = counters_eigen;
		}
		
//		Get number of level-1 samples.
		level1_samples = (int) firstLevel_samples.cols();
		
//		Take keys and hash them all into their intermediate bins. We do this to detect duplicate queries to reduce computational burden.
		std::vector<size_t>* const projected_keys = new std::vector<size_t>(keys->size());
		sensor.topLevelKeyProjection(keys, projected_keys, bypassTopLevel);
		
//		Bitmask that stores for which of the d intermediate counters we need to do level-2 MCMC
		std::vector<bool> computation_needed(hg.num_nodes, false);
		for (size_t projectedKey: *projected_keys) {computation_needed.at(projectedKey) = true;}
		
//		Compute how many level-2 items we need to sample
		const size_t num_computation_needed = std::accumulate(computation_needed.begin(),
															  computation_needed.end(),
															  0);
//		Generate a dense index of which level-2 items need to be sampled
		std::vector<int> computation_needed_idx(num_computation_needed);
		int j = 0;
		for (int i = 0; i < hg.num_nodes; i++) {
			if (computation_needed.at(i) ) {
				computation_needed_idx.at(j) = i;
				j += 1;
			}
		}
		
//		Prune level-1 samples to the set of samples we actually need
		const auto computation_relevant_samples = firstLevel_samples(computation_needed_idx, Eigen::all);
//		Container which returns queried samples
		auto* const return_samples = new std::vector<std::vector<support_type>>(hg.num_nodes);
//		Allocate space for samples we actually need to compute
		for (int i = 0; i < hg.num_nodes; i++) {
			if (computation_needed.at(i) ) {
				return_samples->at(i).reserve(level1_samples * level2_samples);
			}
		}
		
//		If we bypass level-2 sampling, return level-1 samples
		if (bypassTopLevel) {
			for (int i = 0; i < num_computation_needed; i++) {
				const int j = computation_needed_idx.at(i);
				const auto local_samples_ptr = firstLevel_samples.data() + level1_samples * computation_needed_idx[i];
				return_samples->at(j).insert(return_samples->at(j).end(), local_samples_ptr, local_samples_ptr + level1_samples);
			}
//			Return index plus samples. Index is a vector such that teh answer to the k-th query is found in return_samples[projected_keys[k]].
//			We do this to save space and avoid duplicate answers
//			The first samples in the return tuple represents the level1 samples and the second samples the level2 tuples. Since we bypass level2, both are the same.
			return std::make_tuple(projected_keys, return_samples, return_samples);
		}
		
//		Save level1 samples to return
		auto* const level1_return_samples = new std::vector<std::vector<support_type>>(hg.num_nodes);
		for (int i = 0; i < num_computation_needed; i++) {
			const int j = computation_needed_idx.at(i);
			const auto local_samples_ptr = firstLevel_samples.data() + level1_samples * computation_needed_idx[i];
			level1_return_samples->at(j).insert(level1_return_samples->at(j).end(), local_samples_ptr, local_samples_ptr + level1_samples);
		}
		
//		We dont bypass level-2 sampling. Duplicate level-2 priors one for each thread.
		std::array<level2_prior_type, num_threads> level2_prior_array;
		for (int i = 0; i < num_threads; i++) {level2_prior_array.at(i) = *level2_prior;}
		
//		In parallel, compute level-2 samples. We partition workload across the num_computation_needed-many items we sample for
		#pragma omp parallel for num_threads(num_threads)
		for (int i = 0; i < num_computation_needed; i++) {
//			Get thread id
			const int tid = omp_get_thread_num();
//			Get index of which of the d bins we are sampling for
			const int j = computation_needed_idx.at(i);
//			For each level-1 sample for this j-th bin, perform level-2 sampling
			for (int level1_sample_idx = 0; level1_sample_idx < level1_samples; level1_sample_idx++) {
//				This is the counter associated to the sample. It means that counter = A + \sum_i^N X_i, where A is the distribution we want to get the posterior of
				const support_type counter = computation_relevant_samples(i, level1_sample_idx);
//				Expected mean of A conditioned on teh fact that A < counter.
				const double level2_prior_mean_counter = level2_prior->mean(counter);
//				Expected mean of X_i
				const double level2_prior_mean_max = level2_prior->mean(std::numeric_limits<support_type>::max());
//				Estimate expected number of collisions based on counter-size
				const double expected_numCollisions = (counter - level2_prior_mean_counter) / level2_prior_mean_max;
//				Estimate uncertainty about number of collisions based on counter factor. Higher for things like pareto and lower for exponential
				const double variance_numCollisions = expected_numCollisions * level2_prior->variance_factor();
//				Estimate number of collisions based on both the size of the counter as well as the knowledge about the inherent collision probabilities
				const double expected_numCollisions_final = (expected_numCollisions / std::sqrt(variance_numCollisions) + ((cardianlity_estimate - 1) * collision_probability) / std::sqrt((cardianlity_estimate - 1) * collision_probability * (1 - collision_probability))) / (1 / std::sqrt(variance_numCollisions) + 1 / std::sqrt((cardianlity_estimate - 1) * collision_probability * (1 - collision_probability)));
//				We model that the uncertainty about the number of collisions does not change based on the counter.
				const double variance_numCollisions_final =  (cardianlity_estimate - 1) * collision_probability * (1 - collision_probability);
//				If number of collisions is distributed binomially, compute p and n of teh binomial distribution such that we achieve expected_numCollisions_final
				const double bin_p = expected_numCollisions_final / (cardianlity_estimate - 1);
				const double bin_n = (cardianlity_estimate - 1);
//				Compute probability of no collisions. This will be infinitesimally small for cases in which n>>d but it is very necessary when n is not much bigger than d
				const double probability_nocollision_bin = gsl_ran_binomial_pdf(0, bin_p, bin_n);
//				We treat the cases where there are no collisions and at least one collision separately. Here we adjust expectation and variance assuming we have more than zero collisions. The equatiosn are based on simple probability formulas
				const double variance_numCollisions_nocollisionExcluded = ((variance_numCollisions_final + expected_numCollisions_final * expected_numCollisions_final) - expected_numCollisions_final * expected_numCollisions_final / (1 - probability_nocollision_bin)) / (1 - probability_nocollision_bin);
				const double expected_numCollisions_nocollisionExcluded = expected_numCollisions_final / (1 - probability_nocollision_bin);
//				Assumign at least one collision, estimate the mean of \sum_i^N X_i
				const double batchMean = expected_numCollisions_nocollisionExcluded * level2_prior_mean_max;
//				Assumign at least one collision, estimate the variance of \sum_i^N X_i
				const double batchVar = variance_numCollisions_nocollisionExcluded * std::pow(level2_prior_mean_max, 2.0) + expected_numCollisions_nocollisionExcluded * level2_prior->variance(std::numeric_limits<support_type>::max());
//				Set the thread-specific distribution of \sum_i^N X_i such that it has the right expectation and variance
				level2_prior_array[tid].set_batch_approximation(fPlusOne - 1, expected_numCollisions_nocollisionExcluded, batchMean, batchVar);
//				As with the level-1 MCMC, we also need to find an initial-point to start this MCMC chain. We only find one initial point because we do not multithread the MCMC chain
				Eigen::Array<support_type, fPlusOne, 1> initial_point;
				simple_initial_point<fPlusOne>(counter, 0.0, level2_prior_array.data() + tid, gscsi_aux[tid].get_rng(), &initial_point);
//				Set initial value
				gscsi_aux[tid].refresh(&initial_point);
//				MCMC burn in
				gscsi_aux[tid].burnin(counter, level2_prior_array.data() + tid, level2_burnin);
//				Sample
				gscsi_aux[tid].sample(counter, level2_prior_array.data() + tid, level2_period, level2_samples);
//				Retreive samples
				std::vector<support_type> local_samples;
				gscsi_aux[tid].get_samples(&local_samples);
//				Get number of samples generated
				const size_t numsamples_got = local_samples.size();
//				Samplesa re generated assumign at least one collision. We handle the case of zero collisions separately by randomly overwriting the generated samples with probability probability_nocollision_bin with value counter.
//				We do this because if we have no collisions, then N=0 and (counter = A + \sum_i^N X_i) => counter = A
				for (int k = 0; k < numsamples_got; k++) {
					const int indicator = gsl_ran_bernoulli(gscsi_aux[tid].get_rng(), probability_nocollision_bin);
					if (indicator == 1) {local_samples[k] = counter;}
				}
//				Insert samples in return-container
				return_samples->operator[](j).insert(return_samples->operator[](j).end(), local_samples.begin(), local_samples.end());
			}
		}
//		Return index plus samples. Index is a vector such that the answer to the k-th query is found in return_samples[projected_keys[k]].
//		We do this to save space and avoid duplicate answers
//		The first samples in the return tuple represents the level1 samples and teh second samples the level2 tuples.
		return std::make_tuple(projected_keys, level1_return_samples, return_samples);
		
	};
	
//	Clear the sketch
	void refresh_sensor() {
		sensor.clear();
	}
//	Get number of memory cells used by the sketch
	auto getNummemcells() const {
		return sensor.getNummemcells();
	}
//	Get number of fast memory cells used by the sketch
	auto getNummemcellsFast() const {
		return sensor.getNummemcellsFast();
	}
	
	static std::string getDescription() {
		return "BFES";
	}
	
	auto getMemStats() {
		std::cout << "\n-> " + getDescription() + " Memory" << std::endl;
		std::cout << "--> On-Line Memory: " << getNummemcells() << std::endl;
		std::cout << "--> On-Line Counters: " << getNummemcellsFast() << std::endl;
		memstats.clear();
		memstats.push_back(std::make_tuple("OnLineMemory", getNummemcells()));
		memstats.push_back(std::make_tuple("OnLineCounters", getNummemcellsFast()));
		return &memstats;
	}
};

#endif /* McmcSketch_hpp */
